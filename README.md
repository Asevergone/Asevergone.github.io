
# SegNet training and testing scripts

These scripts are for use in training and testing the [SegNet neural
network](http://mi.eng.cam.ac.uk/projects/segnet/), particularly with
OpenStreetMap + Satellite Imagery training data generated by
[skynet-data](https://github.com/developmentseed/skynet-data).

Contributions are very welcome!

# Quick start

The quickest and easiest way to use these scripts is via the
`developmentseed/skynet-train` docker image, but note that to make this work
with a GPU--necessary for reasonable training times---you will need a machine
set up to use [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker).  (The
[start_instance](https://github.com/developmentseed/skynet-train/blob/master/start_instance)
script uses `docker-machine` to spin up an AWS EC2 g2 instance and set it up with
nvidia-docker.)

1. Create a training dataset with [skynet-data](https://github.com/developmentseed/skynet-data).
2. Run:

```sh
nvidia-docker run \
    -v /path/to/training/dataset:/data \
    -v /path/to/training/output:/output \
    -e AWS_ACCESS_KEY_ID=... \
    -e AWS_SECRET_ACCESS_KEY=... \
    developmentseed/skynet-train:gpu \
      --sync s3://your-bucket/training/blahbla
```

This will kick off a training run with the given data. Every 10000 iterations,
the model will be snapshotted and run on the test data, the training "loss"
will be plotted, and all of this uploaded to s3. (Omit the `--sync` argument
and AWS creds to skip the upload.)

Each batch of test results includes a `view.html` file that shows a bare-bones
viewer allowing you to browse the results on a map and compare model outputs to
the ground truth data.  Use it like:
 - http://your-bucket-url/...test-dir.../view.html?imagery_source=MAPID&access_token=MAPBOX_ACCESS_TOKEN where `MAPID` points to Mapbox-hosted raster tiles used for training. (Defaults to `mapbox.satellite`.)
 - http://your-bucket-url/...test-dir.../view.html?imagery_source=http://yourtiles.com/{z}/{x}/{y} for non-Mapbox imagery tiles


Customize the training run with these params:

```
--model MODEL # segnet or segnet_basic, defaults to segnet
--output OUTPUT # directory in which to output training assets
--data DATA # training dataset
[--fetch-data FETCH_DATA] # s3 uri from which to download training data into DATA
[--snapshot SNAPSHOT] # snapshot frequency
[--cpu] # sets cpu mode
[--gpu [GPU [GPU ...]]] # set gpu devices to use
[--display-frequency DISPLAY_FREQUENCY] # frequency of logging output (affects granularity of plots)
[--iterations ITERATIONS] # total number of iterations to run
[--crop CROP] # crop trianing images to CROPxCROP pixels
[--batch-size BATCH_SIZE] # batch size (adjust this up or down based on GPU size. defaults to 6 for segnet and 16 for segnet_basic)
[--sync SYNC]
```


# Details

Prerequisites / Dependencies:
 - Node and Python
 - As of now, training SegNet requires building the [caffe-segnet fork](https://github.com/alexgkendall/caffe-segnet) fork of Caffe.
 - Install node dependencies by running `npm install` in the root directory of this repo.

## Set up model definition

After creating a dataset with the [skynet-data](https://github.com/developmentseed/skynet-data)
scripts, set up the model `prototxt` definition files by running:

```
segnet/setup-model --data /path/to/dataset/ --output /path/to/training/workdir
```

Also copy `segnet/templates/solver.prototxt` to the training work directory, and
edit it to (a) point to the right paths, and (b) set up the learning
"hyperparameters".

(NOTE: this is hard to get right at first; when we post links to a couple of
pre-trained models, we'll also include a copy of the solver.prototxt we used as
a reference / starting point.)


## Train

Download the pre-trained VGG weights `VGG_ILSVRC_16_layers.caffemodel` from
http://www.robots.ox.ac.uk/~vgg/research/very_deep/

From your training work directory, run

```
$CAFFE_ROOT/build/tools/caffe train -gpu 0 -solver solver.txt \
    -weights VGG_ILSVRC_16_layers.caffemodel \
    2>&1 | tee train.log
```

You can monitor the training with:

```
segnet/util/plot_training_log.py train.log --watch
```

This will generate and continually update a plot of the "loss" (i.e., training
error) which should gradually decrease as training progresses.

## Testing the Trained Network

```
segnet/run_test --output /path/for/test/results/ --train /path/to/segnet_train.prototxt --weights /path/to/snapshots/segnet_blahblah_iter_XXXXX.caffemodel --classes /path/to/dataset/classes.json
```

This script essentially carries out the instructions outlined here:
http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html



